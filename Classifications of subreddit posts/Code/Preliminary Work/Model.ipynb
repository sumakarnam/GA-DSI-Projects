{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e243b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# seaborn plot styles\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"deep\")\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "myblue = '#0b5394'\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "#display all output form a cell not just the last (the options are 'all', 'none', 'last' and 'last_expr'.)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfef9142",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/reddit_data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b0c65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>post_char_count</th>\n",
       "      <th>title_char_count</th>\n",
       "      <th>title_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>I would like to dedicate the rest of my 20s to...</td>\n",
       "      <td>(Age 24 from the USA) I've realized that the o...</td>\n",
       "      <td>1630453380</td>\n",
       "      <td>TheSweetOnion</td>\n",
       "      <td>25</td>\n",
       "      <td>298</td>\n",
       "      <td>1688</td>\n",
       "      <td>77</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>Is it difficult because it's Korean, or is it ...</td>\n",
       "      <td>Thank you! Many of you answer my question. \\n\\...</td>\n",
       "      <td>1630450791</td>\n",
       "      <td>Altruistic-Ad-8788</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>249</td>\n",
       "      <td>95</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>Trouble focusing on learning languages.</td>\n",
       "      <td>I’m currently trying to focus on learning Kore...</td>\n",
       "      <td>1630449384</td>\n",
       "      <td>blackholesthrowaway</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>567</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>Any beginner Mandarin speakers who want to cre...</td>\n",
       "      <td>Im searching for a few beginners that learn Ch...</td>\n",
       "      <td>1630447140</td>\n",
       "      <td>zodiacbearexplorer</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>188</td>\n",
       "      <td>66</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>help me choose a language to learn!!</td>\n",
       "      <td>Hi everyone, im an Asian studying in Israel fo...</td>\n",
       "      <td>1630446066</td>\n",
       "      <td>ashleyduong</td>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "      <td>540</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4031</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>Discuss the ways in which power is maintained ...</td>\n",
       "      <td>Not sure why my previous post was erased, very...</td>\n",
       "      <td>1618946058</td>\n",
       "      <td>Myaccountgotbanned12</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>113</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>What's going on with \"indie folk voice\", lingu...</td>\n",
       "      <td>I'm curious about an accent affected by some i...</td>\n",
       "      <td>1618942568</td>\n",
       "      <td>texastential_sm</td>\n",
       "      <td>52</td>\n",
       "      <td>70</td>\n",
       "      <td>448</td>\n",
       "      <td>65</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4033</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>Narrow Transcription of Polish Vowels</td>\n",
       "      <td>What is the narrow transcription of the Polish...</td>\n",
       "      <td>1618940848</td>\n",
       "      <td>CES0803</td>\n",
       "      <td>9</td>\n",
       "      <td>76</td>\n",
       "      <td>501</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4034</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>Discuss the ways in which power is maintained ...</td>\n",
       "      <td>Yes it’s a broad question but I’m interested t...</td>\n",
       "      <td>1618935470</td>\n",
       "      <td>Myaccountgotbanned12</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>Thai word Mirror vs Glass</td>\n",
       "      <td>So is the thai word for mirror and glass the s...</td>\n",
       "      <td>1618933648</td>\n",
       "      <td>ConfusedGrasshopper</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>460</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4036 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit                                              title  \\\n",
       "0     languagelearning  I would like to dedicate the rest of my 20s to...   \n",
       "1     languagelearning  Is it difficult because it's Korean, or is it ...   \n",
       "2     languagelearning            Trouble focusing on learning languages.   \n",
       "3     languagelearning  Any beginner Mandarin speakers who want to cre...   \n",
       "4     languagelearning               help me choose a language to learn!!   \n",
       "...                ...                                                ...   \n",
       "4031       linguistics  Discuss the ways in which power is maintained ...   \n",
       "4032       linguistics  What's going on with \"indie folk voice\", lingu...   \n",
       "4033       linguistics              Narrow Transcription of Polish Vowels   \n",
       "4034       linguistics  Discuss the ways in which power is maintained ...   \n",
       "4035       linguistics                          Thai word Mirror vs Glass   \n",
       "\n",
       "                                               selftext  created_utc  \\\n",
       "0     (Age 24 from the USA) I've realized that the o...   1630453380   \n",
       "1     Thank you! Many of you answer my question. \\n\\...   1630450791   \n",
       "2     I’m currently trying to focus on learning Kore...   1630449384   \n",
       "3     Im searching for a few beginners that learn Ch...   1630447140   \n",
       "4     Hi everyone, im an Asian studying in Israel fo...   1630446066   \n",
       "...                                                 ...          ...   \n",
       "4031  Not sure why my previous post was erased, very...   1618946058   \n",
       "4032  I'm curious about an accent affected by some i...   1618942568   \n",
       "4033  What is the narrow transcription of the Polish...   1618940848   \n",
       "4034  Yes it’s a broad question but I’m interested t...   1618935470   \n",
       "4035  So is the thai word for mirror and glass the s...   1618933648   \n",
       "\n",
       "                    author  num_comments  post_word_count  post_char_count  \\\n",
       "0            TheSweetOnion            25              298             1688   \n",
       "1       Altruistic-Ad-8788             2               36              249   \n",
       "2      blackholesthrowaway             4              100              567   \n",
       "3       zodiacbearexplorer             7               37              188   \n",
       "4              ashleyduong             5              103              540   \n",
       "...                    ...           ...              ...              ...   \n",
       "4031  Myaccountgotbanned12             1               11               64   \n",
       "4032       texastential_sm            52               70              448   \n",
       "4033               CES0803             9               76              501   \n",
       "4034  Myaccountgotbanned12             2               12               68   \n",
       "4035   ConfusedGrasshopper             0               82              460   \n",
       "\n",
       "      title_char_count  title_word_count  \n",
       "0                   77                15  \n",
       "1                   95                16  \n",
       "2                   39                 5  \n",
       "3                   66                11  \n",
       "4                   36                 7  \n",
       "...                ...               ...  \n",
       "4031               113                21  \n",
       "4032                65                 9  \n",
       "4033                37                 5  \n",
       "4034               128                22  \n",
       "4035                25                 5  \n",
       "\n",
       "[4036 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d18c65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, max_features='sqrt')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8325074331020813"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7849355797819624"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "learn              0.033665\n",
       "time               0.030708\n",
       "language           0.027105\n",
       "learning           0.023817\n",
       "linguistics        0.021934\n",
       "                     ...   \n",
       "headphones         0.000000\n",
       "heads              0.000000\n",
       "health             0.000000\n",
       "hear difference    0.000000\n",
       "ות                 0.000000\n",
       "Length: 5000, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using selftext to classify\n",
    "#train-test-split\n",
    "X = data['selftext']\n",
    "y = data['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2), max_features=5000, min_df=2, max_df=0.95)\n",
    "rf = RandomForestClassifier(max_depth=5, max_features='sqrt',random_state = 42)\n",
    "\n",
    "\n",
    "X_train = cv.fit_transform(X_train)\n",
    "X_test = cv.transform(X_test)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_train,y_train)\n",
    "rf.score(X_test, y_test)\n",
    "pd.Series(rf.feature_importances_, cv.get_feature_names()).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2e1c6930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.9], 'cvec__max_features': [2000],\n",
       "                         'cvec__min_df': [2, 3], 'cvec__ngram_range': [(1, 2)],\n",
       "                         'rf__max_depth': [4, 5],\n",
       "                         'rf__max_features': ['sqrt', 0.5],\n",
       "                         'rf__n_estimators': [100, 200]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8377931945820944"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7849355797819624"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest hyperparameter optimization\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())    \n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.9],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'rf__max_depth': [4, 5],\n",
    "    'rf__max_features': ['sqrt', .5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator = pipe,\n",
    "    param_grid = pipe_params,\n",
    "    cv = 5,\n",
    "    n_jobs = -1,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "X = data['selftext']\n",
    "y = data['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, )\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "gs.score(X_train, y_train)\n",
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f4b5bdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 2000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'rf__max_depth': 5,\n",
       " 'rf__max_features': 'sqrt',\n",
       " 'rf__n_estimators': 200}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3913dcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, max_features='sqrt')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7641228939544104"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7185332011892963"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "learn                0.082261\n",
       "learning             0.046202\n",
       "spanish              0.042341\n",
       "linguistics          0.039559\n",
       "language learning    0.039186\n",
       "                       ...   \n",
       "game                 0.000000\n",
       "gaelic               0.000000\n",
       "funny                0.000000\n",
       "friends              0.000000\n",
       "yt                   0.000000\n",
       "Length: 2000, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using title to classify\n",
    "#train-test-split\n",
    "X = data['title']\n",
    "y = data['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2), max_features=2000, min_df=2, max_df=0.95)\n",
    "rf = RandomForestClassifier(max_depth=5, max_features='sqrt', random_state = 42)\n",
    "X_train = cv.fit_transform(X_train)\n",
    "X_test = cv.transform(X_test)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_train,y_train)\n",
    "rf.score(X_test, y_test)\n",
    "pd.Series(rf.feature_importances_, cv.get_feature_names()).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dd338d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, max_features='sqrt', random_state=42)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8490254377271226"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8077304261645193"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "learn                0.079852\n",
       "learning             0.071937\n",
       "language             0.051846\n",
       "app                  0.030625\n",
       "linguistics          0.026370\n",
       "spanish              0.024545\n",
       "listening            0.022876\n",
       "want learn           0.017427\n",
       "practice             0.016913\n",
       "language learning    0.016742\n",
       "learn language       0.015807\n",
       "good                 0.015308\n",
       "vocabulary           0.014392\n",
       "vowel                0.014167\n",
       "want                 0.012612\n",
       "japanese             0.012447\n",
       "duolingo             0.011758\n",
       "day                  0.011149\n",
       "target               0.010575\n",
       "anki                 0.010500\n",
       "time                 0.010342\n",
       "level                0.009990\n",
       "fluent               0.009746\n",
       "learning language    0.008574\n",
       "example              0.008055\n",
       "start                0.008023\n",
       "studying             0.007878\n",
       "b1                   0.007728\n",
       "examples             0.007192\n",
       "german               0.007016\n",
       "feel                 0.006946\n",
       "target language      0.006915\n",
       "tips                 0.006832\n",
       "fluency              0.006715\n",
       "hours                0.006343\n",
       "skills               0.006276\n",
       "improve              0.006086\n",
       "apps                 0.006072\n",
       "research             0.005800\n",
       "wikipedia            0.005488\n",
       "started              0.005451\n",
       "linguistic           0.005364\n",
       "trying learn         0.005351\n",
       "linguists            0.005351\n",
       "dialect              0.005100\n",
       "speak                0.005030\n",
       "native               0.005011\n",
       "french               0.004927\n",
       "vowels               0.004680\n",
       "b2                   0.004668\n",
       "dtype: float64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using title + selftext to classify\n",
    "#train-test-split\n",
    "X = data['title']+ ' ' + data['selftext']\n",
    "y = data['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2), max_features=2000, min_df=2, max_df=0.95)\n",
    "rf = RandomForestClassifier(max_depth=5, max_features='sqrt', random_state = 42)\n",
    "X_train_featurized = cv.fit_transform(X_train)\n",
    "X_test_featurized = cv.transform(X_test)\n",
    "rf.fit(X_train_featurized, y_train)\n",
    "rf.score(X_train_featurized,y_train)\n",
    "rf.score(X_test_featurized, y_test)\n",
    "important_features = pd.Series(rf.feature_importances_, cv.get_feature_names()).sort_values(ascending=False).head(50)\n",
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "34244673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline model:\n",
    "\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "\n",
    "#vectorizing text column and making a DataFrame with the vectors\n",
    "vect = cv.fit_transform(data['title']+data['selftext'])\n",
    "vect_df = pd.DataFrame(vect.todense(), columns=cv.get_feature_names())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4879d072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'languagelearning': ['language',\n",
       "  'learning',\n",
       "  'learn',\n",
       "  'english',\n",
       "  'like',\n",
       "  'spanish',\n",
       "  'know',\n",
       "  'just',\n",
       "  'languages',\n",
       "  've',\n",
       "  'time',\n",
       "  'don',\n",
       "  'words',\n",
       "  'french',\n",
       "  'want',\n",
       "  'native',\n",
       "  'german',\n",
       "  'really',\n",
       "  'speak',\n",
       "  'think',\n",
       "  'people',\n",
       "  'japanese',\n",
       "  'good',\n",
       "  'https',\n",
       "  'help',\n",
       "  'use',\n",
       "  'new',\n",
       "  'level',\n",
       "  'speaking',\n",
       "  'way',\n",
       "  'amp',\n",
       "  'reading',\n",
       "  'read',\n",
       "  'feel',\n",
       "  'lot',\n",
       "  'understand',\n",
       "  'years',\n",
       "  'grammar',\n",
       "  'language learning',\n",
       "  'vocabulary',\n",
       "  'com',\n",
       "  'italian',\n",
       "  'word',\n",
       "  'need',\n",
       "  'start',\n",
       "  'day',\n",
       "  'study',\n",
       "  'trying',\n",
       "  'listening',\n",
       "  'chinese'],\n",
       " 'linguistics': ['language',\n",
       "  'english',\n",
       "  'languages',\n",
       "  'like',\n",
       "  'words',\n",
       "  'know',\n",
       "  'word',\n",
       "  'just',\n",
       "  'linguistics',\n",
       "  'people',\n",
       "  'https',\n",
       "  've',\n",
       "  'amp',\n",
       "  'does',\n",
       "  'different',\n",
       "  'don',\n",
       "  'question',\n",
       "  'example',\n",
       "  'use',\n",
       "  'think',\n",
       "  'sound',\n",
       "  'say',\n",
       "  'used',\n",
       "  'post',\n",
       "  'questions',\n",
       "  'way',\n",
       "  'spanish',\n",
       "  'really',\n",
       "  'native',\n",
       "  'help',\n",
       "  'did',\n",
       "  'ask',\n",
       "  'want',\n",
       "  'german',\n",
       "  'speakers',\n",
       "  'make',\n",
       "  'com',\n",
       "  'time',\n",
       "  'french',\n",
       "  'sounds',\n",
       "  'read',\n",
       "  'gt',\n",
       "  'looking',\n",
       "  'understand',\n",
       "  'speak',\n",
       "  'wikipedia',\n",
       "  'lot',\n",
       "  'good',\n",
       "  'new',\n",
       "  'right']}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "imp_words_dict = {'languagelearning' : [] , 'linguistics' : []}\n",
    "for subreddit in imp_words_dict.keys():    \n",
    "    imp_words_dict[subreddit] = vect_df[data['subreddit'] == subreddit].sum().sort_values(ascending=False).head(50).index.to_list()\n",
    "    \n",
    "imp_words_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8a0e7b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "languagelearning_features = set(imp_words_dict['languagelearning'])-set(imp_words_dict['linguistics'])\n",
    "linguistics_features = set(imp_words_dict['linguistics'])-set(imp_words_dict['languagelearning'])\n",
    "\n",
    "#naive model\n",
    "\n",
    "\n",
    "data['set_of_words'] = (data['title']+data['selftext']).str.split(' ')\n",
    "data['model_1'] = data['set_of_words'].map(lambda x: 'languagelearning' if languagelearning_features.intersection(x) != set() else 'linguistics')\n",
    "data['model_2'] = data['set_of_words'].map(lambda x: 'linguistics' if linguistics_features.intersection(x) != set() else 'languagelearning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1ad365bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>model_1</th>\n",
       "      <th>model_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>languagelearning</td>\n",
       "      <td>linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>linguistics</td>\n",
       "      <td>languagelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>languagelearning</td>\n",
       "      <td>languagelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>languagelearning</td>\n",
       "      <td>linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>languagelearning</td>\n",
       "      <td>languagelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4031</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4033</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "      <td>languagelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4034</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4036 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit           model_1           model_2\n",
       "0     languagelearning  languagelearning       linguistics\n",
       "1     languagelearning       linguistics  languagelearning\n",
       "2     languagelearning  languagelearning  languagelearning\n",
       "3     languagelearning  languagelearning       linguistics\n",
       "4     languagelearning  languagelearning  languagelearning\n",
       "...                ...               ...               ...\n",
       "4031       linguistics       linguistics       linguistics\n",
       "4032       linguistics       linguistics       linguistics\n",
       "4033       linguistics       linguistics  languagelearning\n",
       "4034       linguistics       linguistics       linguistics\n",
       "4035       linguistics       linguistics       linguistics\n",
       "\n",
       "[4036 rows x 3 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     0.711348\n",
       "False    0.288652\n",
       "dtype: float64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     0.571853\n",
       "False    0.428147\n",
       "dtype: float64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['subreddit', 'model_1', 'model_2']]\n",
    "#model1 accuracy\n",
    "\n",
    "for x in ['model_1', 'model_2']:\n",
    "    print(x)\n",
    "    (data[x] == data['subreddit']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c9e32999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#curve of accuracy with n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3db1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
