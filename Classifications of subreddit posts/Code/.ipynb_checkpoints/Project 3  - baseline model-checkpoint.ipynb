{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b2ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# seaborn plot styles\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"deep\")\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "myblue = '#0b5394'\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#display all output form a cell not just the last (the options are 'all', 'none', 'last' and 'last_expr'.)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4fc6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>post_char_count</th>\n",
       "      <th>title_char_count</th>\n",
       "      <th>title_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>I would like to dedicate the rest of my 20s to...</td>\n",
       "      <td>(Age 24 from the USA) I've realized that the o...</td>\n",
       "      <td>1630453380</td>\n",
       "      <td>TheSweetOnion</td>\n",
       "      <td>25</td>\n",
       "      <td>298</td>\n",
       "      <td>1688</td>\n",
       "      <td>77</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>Is it difficult because it's Korean, or is it ...</td>\n",
       "      <td>Thank you! Many of you answer my question. \\n\\...</td>\n",
       "      <td>1630450791</td>\n",
       "      <td>Altruistic-Ad-8788</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>249</td>\n",
       "      <td>95</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit                                              title  \\\n",
       "0  languagelearning  I would like to dedicate the rest of my 20s to...   \n",
       "1  languagelearning  Is it difficult because it's Korean, or is it ...   \n",
       "\n",
       "                                            selftext  created_utc  \\\n",
       "0  (Age 24 from the USA) I've realized that the o...   1630453380   \n",
       "1  Thank you! Many of you answer my question. \\n\\...   1630450791   \n",
       "\n",
       "               author  num_comments  post_word_count  post_char_count  \\\n",
       "0       TheSweetOnion            25              298             1688   \n",
       "1  Altruistic-Ad-8788             2               36              249   \n",
       "\n",
       "   title_char_count  title_word_count  \n",
       "0                77                15  \n",
       "1                95                16  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4036, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>post_char_count</th>\n",
       "      <th>title_char_count</th>\n",
       "      <th>title_word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>languagelearning</th>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linguistics</th>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title  selftext  created_utc  author  num_comments  \\\n",
       "subreddit                                                              \n",
       "languagelearning   2015      2015         2015    2015          2015   \n",
       "linguistics        2021      2021         2021    2021          2021   \n",
       "\n",
       "                  post_word_count  post_char_count  title_char_count  \\\n",
       "subreddit                                                              \n",
       "languagelearning             2015             2015              2015   \n",
       "linguistics                  2021             2021              2021   \n",
       "\n",
       "                  title_word_count  \n",
       "subreddit                           \n",
       "languagelearning              2015  \n",
       "linguistics                   2021  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in already cleaned data\n",
    "\n",
    "data = pd.read_csv('../Data/reddit_data_clean.csv')\n",
    "data.head(2)\n",
    "data.shape\n",
    "data.groupby('subreddit').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cfa92c",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "This section involves building two simple baseline models by considering the most common words that occur in one of the subreddits but not the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0bd552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature and target dfs\n",
    "\n",
    "X = data['title'] + ' ' + data['selftext']\n",
    "# encoding languagelearning subreddit as 0 and linguistics as 1\n",
    "y = data['subreddit'].replace(['languagelearning', 'linguistics'], [0,1]) \n",
    "\n",
    "\n",
    "#train-test-split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, stratify=y)\n",
    "\n",
    "#featurize text columns using count vectorizer\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "X_featurized_train = cv.fit_transform(X_train)\n",
    "X_featurized_test = cv.transform(X_test)\n",
    "\n",
    "X_featurized_train_df = pd.DataFrame(X_featurized_train.todense(), columns=cv.get_feature_names())\n",
    "X_featurized_test_df = pd.DataFrame(X_featurized_test.todense(), columns=cv.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd46cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of most frequent words in each subreddit\n",
    "\n",
    "imp_words_dict = {0 : [] , 1 : []}\n",
    "for subreddit in imp_words_dict.keys():    \n",
    "    imp_words_dict[subreddit] = X_featurized_train_df[y_train.reset_index(drop = True) == subreddit].sum().sort_values(ascending=False).head(50).index.to_list()\n",
    "    \n",
    "#creating sets of distinctive words that occur in one subreddit but not the other\n",
    "\n",
    "languagelearning_distinctive_words = set(imp_words_dict[0])-set(imp_words_dict[1])\n",
    "linguistics_distinctive_words = set(imp_words_dict[1])-set(imp_words_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8adafbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['language',\n",
       "  'learning',\n",
       "  'learn',\n",
       "  'english',\n",
       "  'like',\n",
       "  'spanish',\n",
       "  'know',\n",
       "  'languages',\n",
       "  'just',\n",
       "  'time',\n",
       "  've',\n",
       "  'don',\n",
       "  'words',\n",
       "  'french',\n",
       "  'want',\n",
       "  'native',\n",
       "  'really',\n",
       "  'german',\n",
       "  'speak',\n",
       "  'think',\n",
       "  'japanese',\n",
       "  'https',\n",
       "  'good',\n",
       "  'help',\n",
       "  'people',\n",
       "  'use',\n",
       "  'new',\n",
       "  'amp',\n",
       "  'level',\n",
       "  'way',\n",
       "  'speaking',\n",
       "  'reading',\n",
       "  'feel',\n",
       "  'read',\n",
       "  'lot',\n",
       "  'understand',\n",
       "  'years',\n",
       "  'com',\n",
       "  'start',\n",
       "  'language learning',\n",
       "  'grammar',\n",
       "  'italian',\n",
       "  'vocabulary',\n",
       "  'word',\n",
       "  'day',\n",
       "  'need',\n",
       "  'studying',\n",
       "  'chinese',\n",
       "  'study',\n",
       "  'listening'],\n",
       " 1: ['language',\n",
       "  'english',\n",
       "  'like',\n",
       "  'languages',\n",
       "  'words',\n",
       "  'know',\n",
       "  'word',\n",
       "  'https',\n",
       "  'just',\n",
       "  'people',\n",
       "  'linguistics',\n",
       "  've',\n",
       "  'amp',\n",
       "  'does',\n",
       "  'don',\n",
       "  'question',\n",
       "  'different',\n",
       "  'use',\n",
       "  'example',\n",
       "  'think',\n",
       "  'sound',\n",
       "  'say',\n",
       "  'used',\n",
       "  'post',\n",
       "  'native',\n",
       "  'way',\n",
       "  'spanish',\n",
       "  'questions',\n",
       "  'help',\n",
       "  'speakers',\n",
       "  'german',\n",
       "  'really',\n",
       "  'com',\n",
       "  'did',\n",
       "  'gt',\n",
       "  'ask',\n",
       "  'greek',\n",
       "  'want',\n",
       "  'make',\n",
       "  'time',\n",
       "  'accent',\n",
       "  'sounds',\n",
       "  'french',\n",
       "  'wikipedia',\n",
       "  'speak',\n",
       "  'looking',\n",
       "  'lot',\n",
       "  'new',\n",
       "  'understand',\n",
       "  'read']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'chinese',\n",
       " 'day',\n",
       " 'feel',\n",
       " 'good',\n",
       " 'grammar',\n",
       " 'italian',\n",
       " 'japanese',\n",
       " 'language learning',\n",
       " 'learn',\n",
       " 'learning',\n",
       " 'level',\n",
       " 'listening',\n",
       " 'need',\n",
       " 'reading',\n",
       " 'speaking',\n",
       " 'start',\n",
       " 'study',\n",
       " 'studying',\n",
       " 'vocabulary',\n",
       " 'years'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'accent',\n",
       " 'ask',\n",
       " 'did',\n",
       " 'different',\n",
       " 'does',\n",
       " 'example',\n",
       " 'greek',\n",
       " 'gt',\n",
       " 'linguistics',\n",
       " 'looking',\n",
       " 'make',\n",
       " 'post',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'say',\n",
       " 'sound',\n",
       " 'sounds',\n",
       " 'speakers',\n",
       " 'used',\n",
       " 'wikipedia'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_words_dict\n",
    "languagelearning_distinctive_words\n",
    "linguistics_distinctive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a87b4fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 training accuracy: 0.5156921043937892\n",
      "model1 testing accuracy: 0.5222993062438057\n",
      "model2 training accuracy: 0.4968615791212422\n",
      "model2 testing accuracy: 0.4975222993062438\n"
     ]
    }
   ],
   "source": [
    "#model_1 accuracy\n",
    "    \n",
    "train_preds = X_featurized_train_df[imp_words_dict[0]].sum(axis = 1).map(\n",
    "    lambda x : 0 if x>0 else 1)\n",
    "test_preds = X_featurized_test_df[imp_words_dict[0]].sum(axis = 1).map(\n",
    "    lambda x : 0 if x>0 else 1)\n",
    "\n",
    "print(f'model1 training accuracy: {accuracy_score(y_train, train_preds)}')\n",
    "print(f'model1 testing accuracy: {accuracy_score(y_test, test_preds)}')\n",
    "\n",
    "#model_2 accuracy\n",
    "train_preds = X_featurized_train_df[imp_words_dict[1]].sum(axis = 1).map(\n",
    "    lambda x : 1 if x>0 else 0)\n",
    "test_preds = X_featurized_test_df[imp_words_dict[1]].sum(axis = 1).map(\n",
    "    lambda x : 1 if x>0 else 0)\n",
    "\n",
    "print(f'model2 training accuracy: {accuracy_score(y_train, train_preds)}')\n",
    "print(f'model2 testing accuracy: {accuracy_score(y_test, test_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d84d596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, max_features='sqrt')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8325074331020813"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7849355797819624"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "learn              0.033665\n",
       "time               0.030708\n",
       "language           0.027105\n",
       "learning           0.023817\n",
       "linguistics        0.021934\n",
       "                     ...   \n",
       "headphones         0.000000\n",
       "heads              0.000000\n",
       "health             0.000000\n",
       "hear difference    0.000000\n",
       "ות                 0.000000\n",
       "Length: 5000, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using selftext to classify\n",
    "#train-test-split\n",
    "X = data['selftext']\n",
    "y = data['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2), max_features=5000, min_df=2, max_df=0.95)\n",
    "rf = RandomForestClassifier(max_depth=5, max_features='sqrt',random_state = 42)\n",
    "\n",
    "\n",
    "X_train = cv.fit_transform(X_train)\n",
    "X_test = cv.transform(X_test)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_train,y_train)\n",
    "rf.score(X_test, y_test)\n",
    "pd.Series(rf.feature_importances_, cv.get_feature_names()).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "350746f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.9], 'cvec__max_features': [2000],\n",
       "                         'cvec__min_df': [2, 3], 'cvec__ngram_range': [(1, 2)],\n",
       "                         'rf__max_depth': [4, 5],\n",
       "                         'rf__max_features': ['sqrt', 0.5],\n",
       "                         'rf__n_estimators': [100, 200]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8377931945820944"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7849355797819624"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest hyperparameter optimization\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())    \n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.9],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'rf__max_depth': [4, 5],\n",
    "    'rf__max_features': ['sqrt', .5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator = pipe,\n",
    "    param_grid = pipe_params,\n",
    "    cv = 5,\n",
    "    n_jobs = -1,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "X = data['selftext']\n",
    "y = data['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, )\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "gs.score(X_train, y_train)\n",
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ebae6b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 2000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'rf__max_depth': 5,\n",
       " 'rf__max_features': 'sqrt',\n",
       " 'rf__n_estimators': 200}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "30254690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, max_features='sqrt')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7641228939544104"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7185332011892963"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "learn                0.082261\n",
       "learning             0.046202\n",
       "spanish              0.042341\n",
       "linguistics          0.039559\n",
       "language learning    0.039186\n",
       "                       ...   \n",
       "game                 0.000000\n",
       "gaelic               0.000000\n",
       "funny                0.000000\n",
       "friends              0.000000\n",
       "yt                   0.000000\n",
       "Length: 2000, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using title to classify\n",
    "#train-test-split\n",
    "X = data['title']\n",
    "y = data['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2), max_features=2000, min_df=2, max_df=0.95)\n",
    "rf = RandomForestClassifier(max_depth=5, max_features='sqrt', random_state = 42)\n",
    "X_train = cv.fit_transform(X_train)\n",
    "X_test = cv.transform(X_test)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_train,y_train)\n",
    "rf.score(X_test, y_test)\n",
    "pd.Series(rf.feature_importances_, cv.get_feature_names()).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54ebe1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, max_features='sqrt', random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.85596299966964"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.817641228939544"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "learning              0.059523\n",
       "language              0.040630\n",
       "learn                 0.035188\n",
       "want                  0.030868\n",
       "app                   0.023439\n",
       "linguistics           0.021311\n",
       "level                 0.020917\n",
       "spanish               0.020638\n",
       "examples              0.018316\n",
       "tips                  0.016747\n",
       "language learning     0.016394\n",
       "vocabulary            0.014840\n",
       "b1                    0.014791\n",
       "time                  0.013894\n",
       "day                   0.013357\n",
       "months                0.013090\n",
       "linguistic            0.011835\n",
       "japanese              0.010314\n",
       "week                  0.010287\n",
       "want learn            0.010034\n",
       "speak                 0.009602\n",
       "learn language        0.009309\n",
       "target language       0.009149\n",
       "french                0.008641\n",
       "anki                  0.008517\n",
       "fluent                0.008473\n",
       "trying                0.008378\n",
       "good                  0.007937\n",
       "pronounced            0.007476\n",
       "watch                 0.007206\n",
       "vowel                 0.007105\n",
       "duolingo              0.006890\n",
       "research              0.006825\n",
       "listening             0.006801\n",
       "learning languages    0.006544\n",
       "studying              0.006414\n",
       "apps                  0.006369\n",
       "lessons               0.006313\n",
       "example               0.006310\n",
       "hours                 0.006285\n",
       "practice              0.006188\n",
       "org                   0.005849\n",
       "native                0.005593\n",
       "linguists             0.005461\n",
       "learner               0.005432\n",
       "started               0.005303\n",
       "word                  0.005214\n",
       "years                 0.004999\n",
       "comprehension         0.004936\n",
       "german                0.004929\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using title + selftext to classify\n",
    "#train-test-split\n",
    "X = data['title']+ ' ' + data['selftext']\n",
    "y = data['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2), max_features=5000, min_df=2, max_df=0.95)\n",
    "rf = RandomForestClassifier(max_depth=5, max_features='sqrt', random_state = 42)\n",
    "X_train_featurized = cv.fit_transform(X_train)\n",
    "X_test_featurized = cv.transform(X_test)\n",
    "rf.fit(X_train_featurized, y_train)\n",
    "rf.score(X_train_featurized,y_train)\n",
    "rf.score(X_test_featurized, y_test)\n",
    "important_features = pd.Series(rf.feature_importances_, cv.get_feature_names()).sort_values(ascending=False).head(50)\n",
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "184cd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline model:\n",
    "\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "\n",
    "#vectorizing text column and making a DataFrame with the vectors\n",
    "vect = cv.fit_transform(data['title'] + ' ' + data['selftext'])\n",
    "vect_df = pd.DataFrame(vect.todense(), columns=cv.get_feature_names())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3714043b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'languagelearning': ['language',\n",
       "  'learning',\n",
       "  'learn',\n",
       "  'english',\n",
       "  'like',\n",
       "  'spanish',\n",
       "  'languages',\n",
       "  'know',\n",
       "  'just',\n",
       "  've',\n",
       "  'time',\n",
       "  'don',\n",
       "  'words',\n",
       "  'french',\n",
       "  'want',\n",
       "  'native',\n",
       "  'german',\n",
       "  'really',\n",
       "  'speak',\n",
       "  'think',\n",
       "  'japanese',\n",
       "  'people',\n",
       "  'good',\n",
       "  'help',\n",
       "  'https',\n",
       "  'use',\n",
       "  'level',\n",
       "  'new',\n",
       "  'speaking',\n",
       "  'way',\n",
       "  'amp',\n",
       "  'reading',\n",
       "  'read',\n",
       "  'feel',\n",
       "  'lot',\n",
       "  'understand',\n",
       "  'years',\n",
       "  'grammar',\n",
       "  'language learning',\n",
       "  'vocabulary',\n",
       "  'com',\n",
       "  'italian',\n",
       "  'word',\n",
       "  'need',\n",
       "  'start',\n",
       "  'day',\n",
       "  'study',\n",
       "  'trying',\n",
       "  'listening',\n",
       "  'chinese'],\n",
       " 'linguistics': ['language',\n",
       "  'english',\n",
       "  'languages',\n",
       "  'like',\n",
       "  'words',\n",
       "  'know',\n",
       "  'word',\n",
       "  'just',\n",
       "  'linguistics',\n",
       "  'people',\n",
       "  'https',\n",
       "  've',\n",
       "  'amp',\n",
       "  'does',\n",
       "  'different',\n",
       "  'question',\n",
       "  'don',\n",
       "  'example',\n",
       "  'use',\n",
       "  'think',\n",
       "  'sound',\n",
       "  'say',\n",
       "  'used',\n",
       "  'post',\n",
       "  'questions',\n",
       "  'way',\n",
       "  'spanish',\n",
       "  'help',\n",
       "  'really',\n",
       "  'native',\n",
       "  'ask',\n",
       "  'did',\n",
       "  'want',\n",
       "  'german',\n",
       "  'speakers',\n",
       "  'make',\n",
       "  'com',\n",
       "  'time',\n",
       "  'french',\n",
       "  'sounds',\n",
       "  'read',\n",
       "  'gt',\n",
       "  'looking',\n",
       "  'understand',\n",
       "  'wikipedia',\n",
       "  'speak',\n",
       "  'good',\n",
       "  'lot',\n",
       "  'greek',\n",
       "  'accent']}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "imp_words_dict = {'languagelearning' : [] , 'linguistics' : []}\n",
    "for subreddit in imp_words_dict.keys():    \n",
    "    imp_words_dict[subreddit] = vect_df[data['subreddit'] == subreddit].sum().sort_values(ascending=False).head(50).index.to_list()\n",
    "    \n",
    "imp_words_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bfb1f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "languagelearning_features = set(imp_words_dict['languagelearning'])-set(imp_words_dict['linguistics'])\n",
    "linguistics_features = set(imp_words_dict['linguistics'])-set(imp_words_dict['languagelearning'])\n",
    "\n",
    "#naive model\n",
    "\n",
    "\n",
    "data['set_of_words'] = (data['title']+ ' ' + data['selftext']).str.split(' ')\n",
    "data['model_1'] = data['set_of_words'].map(lambda x: 'languagelearning' if languagelearning_features.intersection(x) != set() else 'linguistics')\n",
    "data['model_2'] = data['set_of_words'].map(lambda x: 'linguistics' if linguistics_features.intersection(x) != set() else 'languagelearning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "79e34b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>model_1</th>\n",
       "      <th>model_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>languagelearning</td>\n",
       "      <td>linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>linguistics</td>\n",
       "      <td>languagelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>languagelearning</td>\n",
       "      <td>languagelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>languagelearning</td>\n",
       "      <td>linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>languagelearning</td>\n",
       "      <td>languagelearning</td>\n",
       "      <td>languagelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4031</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4033</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "      <td>languagelearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4034</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "      <td>linguistics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4036 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit           model_1           model_2\n",
       "0     languagelearning  languagelearning       linguistics\n",
       "1     languagelearning       linguistics  languagelearning\n",
       "2     languagelearning  languagelearning  languagelearning\n",
       "3     languagelearning  languagelearning       linguistics\n",
       "4     languagelearning  languagelearning  languagelearning\n",
       "...                ...               ...               ...\n",
       "4031       linguistics       linguistics       linguistics\n",
       "4032       linguistics       linguistics       linguistics\n",
       "4033       linguistics       linguistics  languagelearning\n",
       "4034       linguistics       linguistics       linguistics\n",
       "4035       linguistics       linguistics       linguistics\n",
       "\n",
       "[4036 rows x 3 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     0.705897\n",
       "False    0.294103\n",
       "dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     0.579286\n",
       "False    0.420714\n",
       "dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['subreddit', 'model_1', 'model_2']]\n",
    "#model1 accuracy\n",
    "\n",
    "for x in ['model_1', 'model_2']:\n",
    "    print(x)\n",
    "    (data[x] == data['subreddit']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c6321243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#curve of accuracy with n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44544062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
