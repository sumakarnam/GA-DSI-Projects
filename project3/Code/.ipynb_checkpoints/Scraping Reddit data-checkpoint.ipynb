{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effd109f",
   "metadata": {},
   "source": [
    "# Scraping Reddit data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36cd04",
   "metadata": {},
   "source": [
    "Using [Pushshift's](https://github.com/pushshift/api) API, 2000+ posts going back from 1st September 2021 were scraped from the following subreddits:\n",
    "\n",
    "- r/languagelearning\n",
    "- r/linguistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78db9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b2808",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "42b75e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subreddit_posts(subreddit, size, before):    \n",
    "\n",
    "    '''\n",
    "    returns a DataFrame with information about a given number (size <= 100) of posts \n",
    "    posted on subreddit before a certain time (before in epoch time)\n",
    "    '''\n",
    "    \n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "    params = {\n",
    "        'subreddit' : subreddit,\n",
    "        'size' : size,\n",
    "        'before': before    \n",
    "    }\n",
    "    res = requests.get(url, params)\n",
    "    df = pd.DataFrame(res.json()['data'])[['subreddit','title','selftext','created_utc', 'author','num_comments']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53981b",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "39afb1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1630314389 0\n",
      "1630163899 52\n",
      "1629974923 108\n",
      "1629807499 190\n",
      "1629671168 246\n",
      "1629518211 295\n",
      "1629376965 345\n",
      "1629223016 397\n",
      "1629094698 457\n",
      "1628966529 500\n",
      "1628803197 548\n",
      "1628671257 614\n",
      "1628531549 652\n",
      "1628422247 701\n",
      "1628269201 749\n",
      "1628134542 793\n",
      "1628009868 842\n",
      "1627907759 894\n",
      "1627766004 946\n",
      "1627663668 1001\n",
      "1627554437 1047\n",
      "1627419486 1093\n",
      "1627291267 1152\n",
      "1627150703 1200\n",
      "1627024031 1252\n",
      "1626895296 1303\n",
      "1626778685 1368\n",
      "1626647039 1437\n",
      "1626509321 1501\n",
      "1626345303 1562\n",
      "1626203936 1616\n",
      "1626083798 1691\n",
      "1625942061 1757\n",
      "1625814793 1822\n",
      "1625682893 1890\n",
      "1625535612 1961\n"
     ]
    }
   ],
   "source": [
    "# Collecting most recent 2000 posts of the subreddit r/lanuguagelearning which haven't \n",
    "#been removed and were posted before 1 Sep 2021 and have some self text\n",
    "#Removed posts and no selftext posts are also included in this scrape\n",
    "\n",
    "languagelearning_posts = pd.DataFrame()\n",
    "num_posts_target = 2000\n",
    "num_posts = 0\n",
    "before = 1630454400 # epoch time 1 Sep 2021 12:00 AM\n",
    "url = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "while num_posts < num_posts_target:\n",
    "    df = scrape_subreddit_posts('languagelearning', 100, before)\n",
    "    time.sleep(20)\n",
    "    languagelearning_posts = languagelearning_posts.append(df, ignore_index=True)\n",
    "    num_removed_posts = len(df[df['selftext'] == '[removed]'])\n",
    "    num_only_title_posts = len(df[df['selftext'] == ''])\n",
    "    before = df['created_utc'].min()\n",
    "    print(before, num_posts)\n",
    "    num_posts += len(df)-num_removed_posts-num_only_title_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "207ca6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving collected data\n",
    "\n",
    "languagelearning_posts.to_csv('../Data/languagelearning.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0a7e0f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1629997573 0\n",
      "1629505503 75\n",
      "1629071769 146\n",
      "1628615216 219\n",
      "1628176613 299\n",
      "1627764834 371\n",
      "1627321642 452\n",
      "1626872310 523\n",
      "1626394923 597\n",
      "1625943876 668\n",
      "1625564249 737\n",
      "1625176336 810\n",
      "1624768602 879\n",
      "1624316732 948\n",
      "1623918746 1029\n",
      "1623570350 1087\n",
      "1623240539 1130\n",
      "1622901231 1199\n",
      "1622558199 1265\n",
      "1622146948 1337\n",
      "1621774828 1415\n",
      "1621383273 1475\n",
      "1620938385 1546\n",
      "1620616384 1617\n",
      "1620247251 1678\n",
      "1619817017 1750\n",
      "1619565511 1814\n",
      "1619207962 1881\n",
      "1618933648 1947\n"
     ]
    }
   ],
   "source": [
    "# Collecting most recent 2000 posts of the subreddit r/linguistics which haven't \n",
    "#been removed and were posted before 1 Sep 2021 and have some self text\n",
    "#Removed posts and no selftext posts are also included in this scrape\n",
    "\n",
    "linguistics_posts = pd.DataFrame()\n",
    "num_posts_target = 2000\n",
    "num_posts = 0\n",
    "before = 1630454400 # epoch time 1 Sep 2021 12:00 AM\n",
    "url = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "while num_posts < num_posts_target:\n",
    "    df = scrape_subreddit_posts('linguistics', 100, before)\n",
    "    time.sleep(20)\n",
    "    linguistics_posts = linguistics_posts.append(df, ignore_index=True)\n",
    "    num_removed_posts = len(df[df['selftext'] == '[removed]'])\n",
    "    num_only_title_posts = len(df[df['selftext'] == ''])\n",
    "    before = df['created_utc'].min()\n",
    "    print(before, num_posts)\n",
    "    num_posts += len(df)-num_removed_posts-num_only_title_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c138eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving collected data\n",
    "\n",
    "linguistics_posts.to_csv('../Data/linguistics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
